{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "360dbb73",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.preprocessing import normalize, MinMaxScaler, StandardScaler\n",
    "from sklearn.decomposition import IncrementalPCA\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "from catboost import CatBoostRegressor, sum_models\n",
    "\n",
    "from typing import Text, Dict, Tuple, List, Callable, Type, Optional\n",
    "\n",
    "import yaml\n",
    "import joblib\n",
    "\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from tqdm.auto import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6817610a",
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = '../config/params.yml'\n",
    "config = yaml.load(open(config_path), Loader=yaml.FullLoader)\n",
    "\n",
    "preproc = config['preprocessing']\n",
    "training = config['train']\n",
    "evaluate = config['evaluate']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85616007",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ddcc153",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dtypes_convert(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Преобразование числовых полей датафрейма к меньшей размерности для экономии вычислительных ресурсов\n",
    "    :param df: датафрейм\n",
    "    \"\"\"\n",
    "    fcols = df.select_dtypes('float').columns\n",
    "    icols = df.select_dtypes('integer').columns\n",
    "\n",
    "    df[fcols] = df[fcols].apply(pd.to_numeric, downcast='float')\n",
    "    df[icols] = df[icols].apply(pd.to_numeric, downcast='unsigned')\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35711ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sg_transform(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Получение данных об их принадлежности к нитям\n",
    "    :param df: датафрейм sensor_geometry\n",
    "    \"\"\"\n",
    "    df_out = df.copy()\n",
    "    # Номер нити, к которой относится датчик\n",
    "    df_out[preproc['line_column']] = df_out[preproc['sensor_column']] // 60 + 1\n",
    "    # Флаг принадлежности датчика к центральным нитям\n",
    "    #df['core'] = (df['line_id'] > 78).astype('uint8')\n",
    "\n",
    "    return df_out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da9a2e9",
   "metadata": {},
   "source": [
    "Импортируем таблицы с метаданными, необходимыми для инференса по тестовому пакету batch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0e222c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sensor geometry data\n",
    "file_path = preproc['file_dirs']['sensor_geometry'][\n",
    "    'local_dir'] + '/' + preproc['file_dirs']['sensor_geometry']['filename']\n",
    "sensor_geometry = pd.read_csv(file_path).set_index(preproc['sensor_column'])\n",
    "\n",
    "# Test meta data\n",
    "file_path = evaluate['file_dirs']['test_meta']['local_dir'] + '/' + evaluate[\n",
    "    'file_dirs']['test_meta']['filename']\n",
    "test_meta = pd.read_parquet(file_path).set_index(preproc['event_column'])\n",
    "test_meta = dtypes_convert(test_meta)\n",
    "\n",
    "del file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75c25faa",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cf4eff",
   "metadata": {},
   "source": [
    "Далее потребуется таблица с пакетом тестовых данных batch, но ее будем импортировать с непосредственной трансформацией данных"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c60d9f7",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee1a9e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cartesian_to_spherical(cart: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Обратное преобразование вектора декартовых координат в сферическую систему\n",
    "    :param cart: масив сферических координат в формате [x, y, z]\n",
    "    \"\"\"\n",
    "    x = cart[:, 0]\n",
    "    y = cart[:, 1]\n",
    "    z = cart[:, 2]\n",
    "    rxy_sq = x**2 + y**2\n",
    "    r = np.sqrt(rxy_sq + z**2)\n",
    "    zenith = np.arctan2(np.sqrt(rxy_sq), z)\n",
    "    zenith = np.where(zenith < 0, zenith + 2 * np.pi, zenith)\n",
    "    azimuth = np.arctan2(y, x)\n",
    "    azimuth = np.where(azimuth < 0, azimuth + 2 * np.pi, azimuth)\n",
    "\n",
    "    return np.array([azimuth, zenith], dtype='float32').T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f5bc446",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten_multiindex(df: pd.DataFrame, isjoin: bool = True) -> list:\n",
    "    \"\"\"\n",
    "    Приведение иерархического мультииндекса стобцов в плоский вид\n",
    "    :param df: датафрейм\n",
    "    :param isjoin: объединение имен через '_', иначе - отбрасываем второй уровень\n",
    "    \"\"\"\n",
    "    if isjoin:\n",
    "        result = [\n",
    "            '_'.join(col).strip()\n",
    "            if len(col[1]) > 0 else ' '.join(col).strip()\n",
    "            for col in df.columns.values\n",
    "        ]\n",
    "    else:\n",
    "        result = [col[0] for col in df.columns.values]\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cddadb96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_prepare(batch_df: pd.DataFrame,\n",
    "                  drop_aux: bool = True,\n",
    "                  doms_agg: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Подготовка батча данных: агрегация импульсов, нормализация времен (отсчет от 0)\n",
    "    :param batch_df: датафрейм с батчем данных импульсов\n",
    "    :param drop_aux: флаг для отброса ненадежных импульсов с auxiliary==True\n",
    "    :param doms_agg: флаг для агрегирования импульсов и их времени по каждому датчику,\n",
    "                     иначе - на выход подаются все необработанные импульсы, в том числе,\n",
    "                     если в одном событии присутствуют импульсы с одного модуля\n",
    "    \"\"\"\n",
    "    if drop_aux:\n",
    "        batch_df = batch_df[batch_df[preproc['aux_column']] == False]\n",
    "    if doms_agg:\n",
    "        batch_df = batch_df.groupby([\n",
    "            preproc['event_column'], preproc['sensor_column']\n",
    "        ]).agg(preproc['prepare_aggregators']).reset_index()\n",
    "        batch_df.columns = flatten_multiindex(batch_df, isjoin=False)\n",
    "    # Находим времена импульсов относительно начала событий\n",
    "    times = batch_df.groupby(preproc['event_column']).agg(\n",
    "        preproc['time_aggregator']).reset_index()\n",
    "    times.columns = flatten_multiindex(times)\n",
    "    min_time_column = list(times.columns)[-1]\n",
    "    batch_df = batch_df.merge(times, on=preproc['event_column'], how='left')\n",
    "    batch_df[preproc['time_column']] = (batch_df[preproc['time_column']] -\n",
    "                                        batch_df[min_time_column])\n",
    "    batch_df.drop(columns=[min_time_column], inplace=True)\n",
    "\n",
    "    return batch_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "592028ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cut_pulses(batch_df: pd.DataFrame,\n",
    "               max_pulses: int = 128,\n",
    "               drop_aux: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Выкидываем последние и ненадёжные импульсы в событии если их больше max_pulses\n",
    "    :param batch_df: датафрейм с импульсами\n",
    "    :param max_pulses: количество импульсов на одно событие (отсечка)\n",
    "    :param drop_aux: флаг для отброса ненадежных импульсов с auxiliary==True\n",
    "    \"\"\"\n",
    "    if drop_aux:\n",
    "        batch_df = batch_df.sort_values(\n",
    "            [preproc['event_column'], preproc['time_column']])\n",
    "    else:\n",
    "        batch_df = batch_df.sort_values([\n",
    "            preproc['event_column'], preproc['aux_column'],\n",
    "            preproc['time_column']\n",
    "        ])\n",
    "    batch_df = batch_df.reset_index(drop=True)\n",
    "    batch_df = batch_df.groupby(preproc['event_column']).head(max_pulses)\n",
    "    batch_df = batch_df.reset_index(drop=True)\n",
    "    if not drop_aux:\n",
    "        batch_df = batch_df.sort_values(\n",
    "            [preproc['event_column'], preproc['time_column']])\n",
    "        batch_df = batch_df.reset_index(drop=True)\n",
    "\n",
    "    return batch_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1563e833",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_event_features(batch_df: pd.DataFrame,\n",
    "                       apply_aux: bool = True) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Набираем агрегированные фичи, характеризующие событие в целом\n",
    "    :param batch_df: датафрейм с импульсами и координатами датчиков\n",
    "    :param apply_aux: флаг обнуления фичей при отсутствии надежных импульсов\n",
    "    \"\"\"\n",
    "    for key, mult in preproc['multiplication'].items():\n",
    "        batch_df[key] = batch_df[mult[0]] * batch_df[mult[1]]\n",
    "\n",
    "    if apply_aux:\n",
    "        for col in batch_df.columns:\n",
    "            if col not in [\n",
    "                    preproc['event_column'], preproc['sensor_column'],\n",
    "                    preproc['line_column']\n",
    "            ]:\n",
    "                batch_df[col] = batch_df[col] * (\n",
    "                    1 - batch_df[preproc['aux_column']])\n",
    "\n",
    "    batch_df = batch_df.groupby(preproc['event_column']).agg(\n",
    "        preproc['aggregators'])\n",
    "    batch_df.columns = flatten_multiindex(batch_df, isjoin=True)\n",
    "    batch_df = batch_df.reset_index()\n",
    "\n",
    "    for key, dision in preproc['divisions'].items():\n",
    "        batch_df[key] = np.log10(batch_df[dision[0]] / batch_df[dision[1]])\n",
    "\n",
    "    for col in preproc['log_scale_transform']:\n",
    "        batch_df[col] = np.log(1 + batch_df[col])\n",
    "\n",
    "    for key, col in preproc['log_features'].items():\n",
    "        batch_df[key] = np.log10(batch_df[col]) / 10\n",
    "\n",
    "    batch_df.drop(columns=preproc['drop_columns'], inplace=True)\n",
    "    # На случай ошибок аггрегирования std значений\n",
    "    batch_df.fillna(value=0, inplace=True)\n",
    "\n",
    "    return batch_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fab6c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_transform(batch_parquet_path: str,\n",
    "                    sensor_geometry: pd.DataFrame,\n",
    "                    meta: pd.DataFrame,\n",
    "                    max_pulses: int = 128,\n",
    "                    drop_aux: bool = True,\n",
    "                    doms_agg: bool = True,\n",
    "                    is_evaluate: bool = False):\n",
    "    \"\"\"\n",
    "    Трансформация батча импульсов в таблицу событий с фичами, характеризующими каждое из них\n",
    "    :param batch_parquet_path: путь к датафрейму батча с импульсами\n",
    "    :param sensor_geometry: датафрейм с геометрией датчиков аппарата IceCube\n",
    "    :param meta: метаданные событиий с векторами направления нейтрино в сф. координатах\n",
    "    :param max_pulses: количество импульсов на одно событие (отсечка)\n",
    "    :param drop_aux: флаг для отброса ненадежных импульсов с auxiliary==True\n",
    "    :param doms_agg: флаг для агрегирования импульсов и их времени по каждому датчику,\n",
    "                     иначе - на выход подаются все необработанные импульсы, в том числе,\n",
    "                     если в одном событии присутствуют импульсы с одного модуля\n",
    "    :param is_evaluate: флаг того, что батч не предназначен для обучения (не имеет таргета)\n",
    "    \"\"\"\n",
    "    batch_df = pd.read_parquet(batch_parquet_path)\n",
    "\n",
    "    # Подготовка данных импульсов перед преобразованием пакета\n",
    "    batch_df = batch_prepare(batch_df=batch_df,\n",
    "                             drop_aux=drop_aux,\n",
    "                             doms_agg=doms_agg)\n",
    "\n",
    "    # Выкидываем последние и ненадёжные импульсы в событии если их больше max_pulses\n",
    "    batch_df = cut_pulses(batch_df=batch_df,\n",
    "                          max_pulses=max_pulses,\n",
    "                          drop_aux=drop_aux)\n",
    "    # Объединяем с геометрией датчиков для получения координат\n",
    "    batch_df = batch_df.merge(sensor_geometry,\n",
    "                              on=preproc['sensor_column'],\n",
    "                              how='left')\n",
    "\n",
    "    # Набираем агрегированные фичи, характеризующие событие в целом\n",
    "    batch_df = get_event_features(batch_df, apply_aux=True)\n",
    "\n",
    "    # Добавляем целевые переменные в сферических и декартовых координатах\n",
    "    if not is_evaluate:\n",
    "        batch_df[preproc['target_columns']] = batch_df.merge(\n",
    "            meta, on=preproc['event_column'],\n",
    "            how='left')[preproc['target_columns']]\n",
    "        # Для снижения нелинейности задачи преобразуем к декартовым координатам\n",
    "        batch_df[preproc['target_cart_columns']] = spherical_to_cartesian(\n",
    "            batch_df[preproc['target_columns']].to_numpy())\n",
    "\n",
    "    # Итоговый датафрейм\n",
    "    batch_df.set_index(preproc['event_column'], drop=True, inplace=True)\n",
    "    batch_df = dtypes_convert(batch_df)\n",
    "\n",
    "    return batch_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09a624f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pipeline_preprocess_evaluate() -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Преобразование всего датачета батчей с импульсами в формат датафреймов для инференса \n",
    "    и сохраняет обработанные файлы в отдельной папке\n",
    "    \"\"\"\n",
    "    sg_file_path = preproc['file_dirs']['sensor_geometry'][\n",
    "        'local_dir'] + '/' + preproc['file_dirs']['sensor_geometry']['filename']\n",
    "    sensor_geometry = dtypes_convert(pd.read_csv(sg_file_path))\n",
    "    sensor_geometry = sg_transform(sensor_geometry)\n",
    "    meta_file_path = evaluate['file_dirs']['test_meta'][\n",
    "        'local_dir'] + '/' + evaluate['file_dirs']['test_meta']['filename']\n",
    "    test_meta = dtypes_convert(pd.read_parquet(meta_file_path))\n",
    "    #test_meta.set_index(preproc['event_column'], inplace=True)\n",
    "\n",
    "    # Проверяем наличие batch-файлов\n",
    "    src_filepath = evaluate['file_dirs']['test_batch']['local_dir']\n",
    "    src_filename = evaluate['file_dirs']['test_batch']['filename']\n",
    "    src_path = src_filepath + src_filename\n",
    "    src_check_file = os.path.isfile(src_path)\n",
    "    if src_check_file:\n",
    "        # Обрабатываем batch-файл и берем из него сэмпл эвентов\n",
    "        batch_data = batch_transform(batch_parquet_path=src_path,\n",
    "                                     sensor_geometry=sensor_geometry,\n",
    "                                     meta=test_meta,\n",
    "                                     max_pulses=10000,\n",
    "                                     drop_aux=True,\n",
    "                                     doms_agg=True,\n",
    "                                     is_evaluate=True)\n",
    "    else:\n",
    "        batch_data = None\n",
    "        print(f'Batch ID #{batch_id} file {src_path} is missing. Skipping')\n",
    "        \n",
    "    return batch_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f63c847",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_df_eval = pipeline_preprocess_evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63016dba",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "batch_df_eval.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27249aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_df_eval.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b21a1f",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e50c524",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = joblib.load(training['model_path'])\n",
    "y_pred = model.predict(batch_df_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b6256f",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60adaf27",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = cartesian_to_spherical(y_pred)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a86085b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = preproc['target_columns']\n",
    "df = pd.DataFrame(prediction, columns=cols, index=batch_df_eval.index, dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd3fb65b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c1bdae",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_filepath = evaluate['file_dirs']['test_batch']['local_dir']\n",
    "src_filename = evaluate['file_dirs']['test_batch']['filename']\n",
    "src_path = src_filepath + src_filename\n",
    "batch_661 = pd.read_parquet(src_path)#.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7443cff3",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_661 = batch_661.groupby([preproc['event_column'], preproc['sensor_column']\n",
    "                   ]).agg(preproc['prepare_aggregators']).reset_index()\n",
    "batch_661.columns = flatten_multiindex(batch_661, isjoin=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70851056",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_661"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ef4d20",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "461px",
    "left": "41px",
    "top": "111.141px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
